# environment the application is running
# c5yarn.gbif.org, but the alias isn't accepted
hadoop.jobtracker=c5master2-vh.gbif.org:8032
hdfs.namenode=hdfs://ha-nn
oozie.url=http://c5oozie.gbif.org:11000/oozie

# Uses oozies shared lib and the /lib folder in the workflow
oozie.use.system.libpath=true

# hdfs because we do things like chown hbase:hbase
user.name=hdfs

# location of the workflow and jars
#oozie.wf.application.path=hdfs://ha-nn/maps-backfill-workflow
oozie.coord.application.path=hdfs://ha-nn/maps-backfill-workflow/coordinator-points.xml
oozie.libpath=hdfs://ha-nn/maps-backfill-workflow/lib/

# fine tune the spark submission
gbif.map.spark.opts=--driver-memory 4G --executor-memory 36G --num-executors 30 --executor-cores 10 --conf spark.yarn.executor.memoryOverhead=4096

# the name of the file for the configuration
gbif.map.spark.conf=prod.yml

# ZK Quorum for the HBase cluster
gbif.map.zk.quorum=c5zk1.gbif.org:2181,c5zk2.gbif.org:2181,c5zk3.gbif.org:2181

# Source HBase occurrence table to snapshot for the input
gbif.map.sourceTable=prod_e_occurrence

# The target table is finalised as prefix_mode_timestamp (e.g. dev_maps_tiles_201707_01_1315
gbif.map.targetTablePrefix=prod_e_maps

# The target directory for the HFiles
gbif.map.targetDirectory=hdfs://ha-nn/tmp/prod_e_maps

# Whether to run the point or the tile backfill
gbif.map.mode=points
#gbif.map.mode=tiles

# The modulus controls the number of regions in the target table (sensible defaults are 5-10 for dev and 100 for prod)
gbif.map.keySaltModulus=100

# The ZK Path where the table names are located
gbif.map.zk.metadataPath=/prod_maps/meta
